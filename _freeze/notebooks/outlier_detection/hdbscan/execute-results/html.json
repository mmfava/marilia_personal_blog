{
  "hash": "9632dd20332ef70082ef13baec4448f3",
  "result": {
    "markdown": "---\ntitle: HDBSCAN\n---\n\nO algoritmo [**`HDBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.HDBSCAN.html#sklearn.cluster.HDBSCAN) pode ser visto como uma extens√£o de [**`DBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN) e [**`OPTICS`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html#sklearn.cluster.OPTICS). Especificamente, [**`DBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN) assume que o crit√©rio de agrupamento (ou seja, requisito de densidade) √© *globalmente homog√™neo*. Em outras palavras, [**`DBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN) pode ter dificuldades para capturar com sucesso clusters com diferentes densidades. [**`HDBSCAN`**](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.HDBSCAN.html#sklearn.cluster.HDBSCAN) alivia essa suposi√ß√£o e explora todas as poss√≠veis escalas de densidade construindo uma representa√ß√£o alternativa do problema de agrupamento.\n\n## **[1] Defini√ß√£o dos pontos *regulares* e *core*.**\n\n<aside>\nüìå  Considerado *core* se k-vizinhos pr√≥ximos considerando uma dist√¢ncia $d$.\n\n</aside>\n\n**Core Points** s√£o pontos centrais em um conjunto de dados que possuem uma densidade local suficientemente alta. Para determinar se um ponto $x$ √© um core point, verificamos se h√° pelo menos um n√∫mero m√≠nimo de pontos $\\text{min\\_samples}$ dentro de um determinado raio, conhecido como **core distance** $\\mathrm{core}_k(x)$. A core distance √© a dist√¢ncia at√© o $k-√©simo$ vizinho mais pr√≥ximo de $x$. Se $x$ tem pelo menos $\\text{min\\_samples}$ pontos dentro de sua core distance, ele √© considerado um core point; caso contr√°rio, √© um ponto regular.\n\n![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/b7f16c67-bf03-415d-918a-e4bf5c83e015/5303eece-c137-4eb4-bd61-0b073e0e014c/Untitled.png)\n\n## **[2] Dist√¢ncia de alcance m√∫tuo**\n\nA dist√¢ncia de alcance m√∫tuo $d_{\\mathrm{mreach-}k}(a, b)$ √© usada para garantir que apenas pontos de densidades similares sejam conectados, tornando a clusteriza√ß√£o mais robusta ao ru√≠do e √† variabilidade dos dados. Ela √© definida como:\n\n$$\n‚Åç\n\n$$\n\n**Onde:**\n\n$\\mathrm{core}_k(a)$ √© a *core distance* do ponto a\n\n$\\mathrm{core}_k(b)$ √© a *core distance* do ponto b\n\n$d(a, b)$ √© a dist√¢ncia euclidiana entre a e b\n\n<aside>\nüìå Se um ponto n√£o √© core, a f√≥rmula √© definida como:\n$d_{\\mathrm{mreach-}k}(a, b) = \\max \\{\\mathrm{core}_k(a), d(a, b)\\}$\n\n</aside>\n\n### Exemplo 1\n\n**IMPORTANTE**: \n\n- A dist√¢ncia de alcance m√∫tuo n√£o s√≥ leva em considera√ß√£o a proximidade espacial (dist√¢ncia euclidiana) entre pontos, mas tamb√©m a densidade local dos pontos, refletida pelas *core distances*. Isso ajuda a garantir que clusters s√≥ se formem se os pontos n√£o s√≥ estiverem pr√≥ximos uns dos outros, mas tamb√©m tiverem densidades similares.\n- A variabilidade dos dados dentro de um cluster √© refletida na core distance. Clusters com alta densidade (baixa variabilidade) ter√£o pequenas core distances, enquanto clusters menos densos (alta variabilidade) ter√£o core distances maiores. Ao usar a dist√¢ncia de alcance m√∫tuo, estamos efetivamente medindo a compatibilidade dos clusters em termos de densidade e variabilidade, al√©m da proximidade espacial.\n\n![A dist√¢ncia entre os pontos azul e verde √© menor que a *core distance* do verde, representando densidades distintas. ](https://prod-files-secure.s3.us-west-2.amazonaws.com/b7f16c67-bf03-415d-918a-e4bf5c83e015/580d8e17-f52e-45e0-a274-c5725617799b/Untitled.png)\n\nA dist√¢ncia entre os pontos azul e verde √© menor que a *core distance* do verde, representando densidades distintas. \n\n![Se a *core distance* do ponto azul √© 0.3 (eixo x: 1.5 - 1.2), a do ponto verde √© 0.7 (eixo x: 2.2 - 1.5), e a dist√¢ncia direta entre azul e verde $d$ √© 0.4, ent√£o a dist√¢ncia de alcance m√∫tuo $d_{\\mathrm{mreach-}k}$ √© determinada pelo maior valor dentre esses tr√™s valores. Neste caso, a maior dist√¢ncia √© 0.7 (do conjunto $[0.3, 0.7, 0.4]$), logo, a dist√¢ncia de alcance m√∫tuo ser√° 0.7.](https://prod-files-secure.s3.us-west-2.amazonaws.com/b7f16c67-bf03-415d-918a-e4bf5c83e015/63af32cc-c64e-471a-8ea7-742b572ced7a/Untitled.png)\n\nSe a *core distance* do ponto azul √© 0.3 (eixo x: 1.5 - 1.2), a do ponto verde √© 0.7 (eixo x: 2.2 - 1.5), e a dist√¢ncia direta entre azul e verde $d$ √© 0.4, ent√£o a dist√¢ncia de alcance m√∫tuo $d_{\\mathrm{mreach-}k}$ √© determinada pelo maior valor dentre esses tr√™s valores. Neste caso, a maior dist√¢ncia √© 0.7 (do conjunto $[0.3, 0.7, 0.4]$), logo, a dist√¢ncia de alcance m√∫tuo ser√° 0.7.\n\n![Neste segundo exemplo, a core distance do grupo vermelho √© 0,5 e do verde 0,7, e a dist√¢ncia direta entre vermelho e verde $d$ √© 0,8. Neste caso, a maior dist√¢ncia √© 0,8 (do conjunto $[0.5, 0.7, 0.8]$), logo, a dist√¢ncia de alcance m√∫tuo ser√° 0,8.](https://prod-files-secure.s3.us-west-2.amazonaws.com/b7f16c67-bf03-415d-918a-e4bf5c83e015/92fecfb1-ff26-421a-a4dc-f523e314cda5/Untitled.png)\n\nNeste segundo exemplo, a core distance do grupo vermelho √© 0,5 e do verde 0,7, e a dist√¢ncia direta entre vermelho e verde $d$ √© 0,8. Neste caso, a maior dist√¢ncia √© 0,8 (do conjunto $[0.5, 0.7, 0.8]$), logo, a dist√¢ncia de alcance m√∫tuo ser√° 0,8.\n\n### Exemplo 2\n\nSuponha que definimos $k = 3$ e temos os seguintes pontos: $A$, $B$, $C$, $D$, e $E$.\n\n**Dist√¢ncias Euclidianas**\n\n| Par de Pontos | Dist√¢ncia Euclidiana |\n| --- | --- |\n| A - B | 2 |\n| A - C | 4 |\n| A - D | 6 |\n| A - E | 8 |\n| B - C | 3 |\n| B - D | 5 |\n| B - E | 7 |\n| C - D | 2 |\n| C - E | 4 |\n| D - E | 1 |\n\n**An√°lise dos Pontos**\n\n1. **Ponto A**:\n    - Vizinhos mais pr√≥ximos: B (2), C (4), D (6)\n    - Core distance ($k = 3$): 6\n    - $A$ √© um core point.\n2. **Ponto B**:\n    - Vizinhos mais pr√≥ximos: A (2), C (3), D (5)\n    - Core distance ($k = 3$): 5\n    - $B$ √© um core point.\n3. **Ponto C**:\n    - Vizinhos mais pr√≥ximos: D (2), B (3), A (4)\n    - Core distance ($k = 3$): 4\n    - $C$ √© um core point.\n4. **Ponto D**:\n    - Vizinhos mais pr√≥ximos: E (1), C (2), B (5)\n    - Core distance ($k = 3$): 5\n    - $D$ √© um core point.\n5. **Ponto E**:\n    - Vizinhos mais pr√≥ximos: D (1), C (4), B (7)\n    - Core distance ($k = 3$): 7\n    - $E$ √© um core point.\n\n**Caso com Menos de $k$ Vizinhos**\n\nSuponha agora que temos um novo ponto $F$ com apenas dois vizinhos:\n\n| Par de Pontos | Dist√¢ncia Euclidiana |\n| --- | --- |\n| F - A | 3 |\n| F - B | 5 |\n1. **Ponto F**:\n    - Vizinhos mais pr√≥ximos: A (3), B (5)\n    - N√£o h√° um terceiro vizinho.\n    - $F$ n√£o pode ser um core point.\n    \n\n**Tratamento de Pontos N√£o-Core**\n\n- **Dist√¢ncia de Alcance M√∫tuo**:\n    - Mesmo que $F$ n√£o seja um core point, ainda calculamos a dist√¢ncia de alcance m√∫tuo para todos os pontos $a$ e $b$, incluindo $F$.\n    - A core distance de $F$ n√£o ser√° usada, mas consideramos as core distances dos outros pontos.\n1. **Dist√¢ncia de Alcance M√∫tuo**:\n    - Para $F$ e qualquer outro ponto, a dist√¢ncia de alcance m√∫tuo ser√°:\n    $d_{\\mathrm{mreach-}k}(F, a) = \\max \\{\\mathrm{core}_k(a), d(F, a)\\}$\n    - Como $F$ n√£o tem uma core distance v√°lida, usamos apenas a core distance do ponto $a$ e a dist√¢ncia euclidiana entre $F$ e $a$.\n\n## **[3] Minimum Spanning Tree no HDBSCAN**\n\nNo contexto do HDBSCAN, a *Minimum Spanning Tree* (MST) desempenha um papel crucial na identifica√ß√£o e forma√ß√£o de clusters. A MST √© uma estrutura que conecta todos os pontos de um conjunto de dados atrav√©s de arestas (conex√µes) de tal forma que o custo total (soma das dist√¢ncias das arestas) seja minimizado, e n√£o haja ciclos.\n\n### **Propriedades da MST**\n\n1. **Conectividade**: A MST conecta todos os v√©rtices do grafo original.\n2. **Arestas M√≠nimas**: A MST tem V‚àí1 arestas, onde V √© o n√∫mero de v√©rtices.\n3. **Peso M√≠nimo**: A soma dos pesos das arestas na MST √© a menor poss√≠vel entre todas as √°rvores geradoras do grafo.\n4. **Sem Ciclos**: A MST √© uma √°rvore e, portanto, n√£o cont√©m ciclos.\n\nExistem v√°rios algoritmos para encontrar a MST em um grafo. Os dois mais comuns s√£o o **Algoritmo de Prim** e o **Algoritmo de Kruskal:** \n\n- O Algoritmo de Prim constr√≥i a MST a partir de um √∫nico v√©rtice, adicionando arestas uma a uma at√© que todos os v√©rtices estejam conectados.\n- O Algoritmo de Kruskal constr√≥i a MST adicionando as arestas de menor peso uma a uma, garantindo que nenhum ciclo seja formado.\n\nA MST pode ser visualizada como um dendrograma, onde a altura das linhas verticais representa a dist√¢ncia de alcance m√∫tuo na qual os pontos est√£o conectados. Esse dendrograma √© uma representa√ß√£o visual da hierarquia de clusters.\n\n![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/b7f16c67-bf03-415d-918a-e4bf5c83e015/27f80c55-65ae-4a73-9e79-365a8e5a1810/Untitled.png)\n\n### **Exemplo**\n\nSuponha um grafo com 5 v√©rtices A, B, C, D, E) e as seguintes arestas e pesos:\n\n| Aresta | Peso |\n| --- | --- |\n| A - B | 2 |\n| A - C | 3 |\n| A - D | 6 |\n| B - C | 1 |\n| B - D | 4 |\n| B - E | 5 |\n| C - D | 2 |\n| C - E | 3 |\n| D - E | 4 |\n\n**Algoritmo de Prim**\n\n1. **Inicializar**:\n    - Comece com o v√©rtice A.\n2. **Adicionar Aresta M√≠nima**:\n    - Arestas conectadas a A: A - B (2), A - C (3), A - D (6).\n    - Aresta de menor peso: A - B (2).\n3. **Adicionar Pr√≥ximo V√©rtice e Aresta M√≠nima**:\n    - V√©rtices na √°rvore: A, B.\n    - Arestas conectadas a B: B - C (1), B - D (4), B - E (5).\n    - Aresta de menor peso: B - C (1).\n4. **Continuar Adicionando Arestas**:\n    - V√©rtices na √°rvore: A, B, C.\n    - Arestas conectadas a C: C - D (2), C - E (3).\n    - Aresta de menor peso: C - D (2).\n5. **Adicionar √öltima Aresta**:\n    - V√©rtices na √°rvore: A, B, C, D.\n    - Arestas conectadas a D: D - E (4).\n    - Aresta de menor peso: C - E (3).\n    \n\n**MST Resultante**\n\n| Aresta | Peso |\n| --- | --- |\n| A - B | 2 |\n| B - C | 1 |\n| C - D | 2 |\n| C - E | 3 |\n\nPeso total da MST: 2 + 1 + 2 + 3 = 8.\n\n**Visualiza√ß√£o da MST**\n\nPodemos visualizar a MST resultante como:\n\n```\n  A -- 2 -- B\n        |    |\n        1    4\n        |    |\n        C -- 2 -- D\n        |\n        3\n        |\n        E\n\n```\n\n**Em c√≥digo:**\n\n```python\nclusterer.minimum_spanning_tree_.plot(edge_cmap='viridis',\n                                      edge_alpha=0.6,\n                                      node_size=80,\n                                      edge_linewidth=2)\n```\n\n![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/b7f16c67-bf03-415d-918a-e4bf5c83e015/27f80c55-65ae-4a73-9e79-365a8e5a1810/Untitled.png)\n\n## [4] Hierarquia do cluster\n\nA hierarquia de clusters no HDBSCAN √© uma representa√ß√£o em √°rvore que captura a estrutura de agrupamento dos dados em diferentes escalas de densidade. Ao contr√°rio de m√©todos tradicionais de clusteriza√ß√£o que produzem uma √∫nica parti√ß√£o dos dados, o HDBSCAN constr√≥i uma hierarquia que reflete como os clusters emergem e se dividem √† medida que a densidade m√≠nima necess√°ria para formar um cluster varia.\n\n![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/b7f16c67-bf03-415d-918a-e4bf5c83e015/9ecf5eee-d9b5-4df5-b553-2ce7173c6b37/Untitled.png)\n\n### **Exemplo**\n\nSuponha que temos um conjunto de pontos \\(\\{P1, P2, P3, P4, P5, P6\\}\\) com as seguintes dist√¢ncias entre eles:\n\n| Ponto 1 | Ponto 2 | Dist√¢ncia |\n| --- | --- | --- |\n| P1 | P2 | 1 |\n| P1 | P3 | 3 |\n| P1 | P4 | 4 |\n| P1 | P5 | 5 |\n| P1 | P6 | 2 |\n| P2 | P3 | 2 |\n| P2 | P4 | 4 |\n| P2 | P5 | 3 |\n| P2 | P6 | 3 |\n| P3 | P4 | 1 |\n| P3 | P5 | 2 |\n| P3 | P6 | 3 |\n| P4 | P5 | 4 |\n| P4 | P6 | 5 |\n| P5 | P6 | 3 |\n\n**Passo 1: Construir a MST**\n\nPrimeiro, constru√≠mos a MST usando o Algoritmo de Kruskal. Vamos adicionar as arestas em ordem crescente de peso, evitando ciclos.\n\n1. **Ordenar as Arestas**:\n    - (P1, P2): 1\n    - (P3, P4): 1\n    - (P1, P6): 2\n    - (P2, P3): 2\n    - (P3, P5): 2\n    - (P2, P5): 3\n    - (P2, P6): 3\n    - (P3, P6): 3\n    - (P5, P6): 3\n    - (P1, P3): 3\n    - (P4, P5): 4\n    - (P1, P4): 4\n    - (P2, P4): 4\n    - (P4, P6): 5\n    - (P1, P5): 5\n2. **Adicionar Arestas √† MST**:\n    - Aresta (P1, P2): 1\n    - Aresta (P3, P4): 1\n    - Aresta (P1, P6): 2\n    - Aresta (P2, P3): 2\n    - Aresta (P3, P5): 2\n\nA MST resultante √©:\n\n```\nP1 -- 1 -- P2\n|         |\n2         2\n|         |\nP6       P3 -- 1 -- P4\n         |\n         2\n         |\n         P5\n\n```\n\n**Passo 2: Construir a Hierarquia de Clusters**\n\nVamos construir a hierarquia de clusters a partir da MST, ordenando as arestas por dist√¢ncia e iterando sobre elas para unir os clusters.\n\n1. **Inicializar Estrutura Union-Find**:\n    - Cada ponto come√ßa como seu pr√≥prio cluster.\n    - Clusters iniciais: \\(\\{P1\\}, \\{P2\\}, \\{P3\\}, \\{P4\\}, \\{P5\\}, \\{P6\\}\\)\n2. **Iterar Sobre as Arestas Ordenadas**:\n    - Aresta (P1, P2): 1\n        - Une \\(\\{P1\\}\\) e \\(\\{P2\\}\\) em \\(\\{P1, P2\\}\\)\n        - Hierarquia: \\(\\{P1, P2\\}, \\{P3\\}, \\{P4\\}, \\{P5\\}, \\{P6\\}\\)\n    - Aresta (P3, P4): 1\n        - Une \\(\\{P3\\}\\) e \\(\\{P4\\}\\) em \\(\\{P3, P4\\}\\)\n        - Hierarquia: \\(\\{P1, P2\\}, \\{P3, P4\\}, \\{P5\\}, \\{P6\\}\\)\n    - Aresta (P1, P6): 2\n        - Une \\(\\{P1, P2\\}\\) e \\(\\{P6\\}\\) em \\(\\{P1, P2, P6\\}\\)\n        - Hierarquia: \\(\\{P1, P2, P6\\}, \\{P3, P4\\}, \\{P5\\}\\)\n    - Aresta (P2, P3): 2\n        - Une \\(\\{P1, P2, P6\\}\\) e \\(\\{P3, P4\\}\\) em \\(\\{P1, P2, P3, P4, P6\\}\\)\n        - Hierarquia: \\(\\{P1, P2, P3, P4, P6\\}, \\{P5\\}\\)\n    - Aresta (P3, P5): 2\n        - Une \\(\\{P1, P2, P3, P4, P6\\}\\) e \\(\\{P5\\}\\) em \\(\\{P1, P2, P3, P4, P5, P6\\}\\)\n        - Hierarquia: \\(\\{P1, P2, P3, P4, P5, P6\\}\\)\n3. **Hierarquia de Clusters**:\n    - A hierarquia resultante pode ser visualizada como um dendrograma:\n\n```\nP1\n|\nP2\n|\nP6\n|\nP3\n|\nP4\n|\nP5\n\n```\n\n**Visualiza√ß√£o da Hierarquia**\n\nPodemos visualizar a hierarquia resultante como um dendrograma, onde a altura das linhas verticais representa a dist√¢ncia de alcance m√∫tuo na qual os pontos est√£o conectados.\n\n```python\nfrom scipy.cluster.hierarchy import dendrogram\nimport numpy as np\n\n# Exemplo de matriz de liga√ß√£o para visualizar o dendrograma\nlinkage_matrix = np.array([\n    [0, 1, 1, 2],  # P1 - P2\n    [2, 3, 1, 2],  # P3 - P4\n    [0, 4, 2, 3],  # P1 - P6\n    [1, 5, 2, 3],  # P2 - P3\n    [2, 6, 2, 3]   # P3 - P5\n])\n\n# Plot do dendrograma\nplt.figure(figsize=(10, 7))\ndendrogram(linkage_matrix, labels=['P1', 'P2', 'P3', 'P4', 'P5', 'P6'])\nplt.title(\"Dendrograma da Hierarquia de Clusters\")\nplt.xlabel(\"Pontos\")\nplt.ylabel(\"Dist√¢ncia de Alcance M√∫tuo\")\nplt.show()\n```\n\n![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/b7f16c67-bf03-415d-918a-e4bf5c83e015/a731f997-dc2e-490f-a25b-c16989c0d83b/Untitled.png)\n\n## [5] Condensar a hierarquia de cluster\n\nO primeiro passo na extra√ß√£o de clusters √© condensar a grande e complicada hierarquia de clusters em uma √°rvore menor com um pouco mais de dados anexados a cada n√≥. Como voc√™ pode ver na hierarquia acima, √© frequentemente o caso de uma divis√£o de cluster ser um ou dois pontos se separando de um cluster; e esse √© o ponto-chave ‚Äì em vez de v√™-lo como um cluster se dividindo em dois novos clusters, queremos v√™-lo como um √∫nico cluster persistente que est√° ‚Äòperdendo pontos‚Äô. Para tornar isso concreto, precisamos de uma no√ß√£o de¬†**tamanho m√≠nimo do cluster**¬†que tomamos como um par√¢metro para HDBSCAN. Uma vez que temos um valor para o tamanho m√≠nimo do cluster, podemos agora percorrer a hierarquia e em cada divis√£o perguntar se um dos novos clusters criados pela divis√£o tem menos pontos do que o tamanho m√≠nimo do cluster. Se for o caso de termos menos pontos do que o tamanho m√≠nimo do cluster, declaramos que s√£o ‚Äòpontos caindo fora de um cluster‚Äô e fazemos o cluster maior manter a identidade do cluster do pai, anotando quais pontos ‚Äòca√≠ram do cluster‚Äô e em que valor de dist√¢ncia isso aconteceu. Por outro lado, se a divis√£o for em dois clusters, cada um com pelo menos o tamanho m√≠nimo do cluster, ent√£o consideramos isso uma verdadeira divis√£o de cluster e deixamos essa divis√£o persistir na √°rvore. Depois de percorrer toda a hierarquia e fazer isso, acabamos com uma √°rvore muito menor com um n√∫mero pequeno de n√≥s, cada um dos quais tem dados sobre como o tamanho do cluster naquele n√≥ diminui ao longo de v√°rias dist√¢ncias. Podemos visualizar isso como um dendrograma semelhante ao acima ‚Äì novamente podemos fazer a largura da linha representar o n√∫mero de pontos no cluster. Desta vez, no entanto, essa largura varia ao longo do comprimento da linha √† medida que os pontos caem do cluster. Para nossos dados, usando um tamanho m√≠nimo de cluster de 5, o resultado se parece com isto:\n\n![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/b7f16c67-bf03-415d-918a-e4bf5c83e015/4d421e60-05e0-4d63-a6c6-7fe21d1a51c2/Untitled.png)\n\n- A MST √© filtrada para criar a hierarquia. A filtragem envolve a remo√ß√£o iterativa das arestas mais longas, o que corresponde a aumentar o limiar de densidade. Ao fazer isso, a √°rvore se divide em sub√°rvores menores, correspondendo a clusters menores e mais densos.\n- A hierarquia completa de clusters √© condensada para focar apenas nas divis√µes significativas. Isso √© feito usando um par√¢metro de tamanho m√≠nimo de cluster (`min_cluster_size`). Divis√µes que resultam em clusters menores que esse tamanho s√£o descartadas.\n- A partir da hierarquia condensada, os clusters finais s√£o selecionados. O m√©todo de sele√ß√£o pode ser baseado no excesso de massa (excess of mass, EOM) ou na folha (leaf).\n- O m√©todo EOM √© geralmente preferido por sua robustez e estabilidade na forma√ß√£o dos clusters.\n\n### Exemplo\n\nCondensa√ß√£o da √Årvore de Clusters no HDBSCAN\n\nConceito de Condensa√ß√£o\n\nNo HDBSCAN, a condensa√ß√£o da √°rvore de clusters envolve simplificar a hierarquia de clusters gerada pelo algoritmo para focar em divis√µes significativas, ou seja, aquelas que resultam em clusters robustos e persistentes. As divis√µes s√£o baseadas nos valores de dist√¢ncia de alcan√ßabilidade m√∫tuas na MST.\n\nPasso a Passo Detalhado\n\n1. **Constru√ß√£o da MST**: A MST conecta todos os pontos com o menor peso total poss√≠vel.\n2. **Hierarquia de Clusters**: Ordene as arestas da MST e crie a hierarquia de clusters.\n3. **Condensa√ß√£o da √Årvore**: Use um tamanho m√≠nimo de cluster para identificar e remover divis√µes insignificantes.\n\n**Exemplo Num√©rico**\n\n**MST Constru√≠da**\n\nConsidere os seguintes pontos e dist√¢ncias na MST:\n\n| Ponto 1 | Ponto 2 | Dist√¢ncia |\n| --- | --- | --- |\n| P1 | P2 | 1 |\n| P1 | P3 | 2 |\n| P1 | P4 | 3 |\n| P2 | P5 | 4 |\n| P3 | P6 | 5 |\n| P4 | P7 | 6 |\n| P5 | P8 | 7 |\n| P6 | P9 | 8 |\n\n**Divis√£o Baseada na MST**\n\nPara construir a hierarquia de clusters, come√ßamos com a MST completa e iterativamente removemos as arestas mais longas.\n\n1. **Hierarquia Inicial**:\n    - Clusters: \\(\\{P1, P2, P3, P4, P5, P6, P7, P8, P9\\}\\)\n2. **Primeira Divis√£o (Remover P6-P9)**:\n    - Clusters: \\(\\{P1, P2, P3, P4, P5, P6, P7, P8\\}\\) e \\(\\{P9\\}\\)\n    - Como \\(\\{P9\\}\\) tem menos de 5 pontos, √© considerado ru√≠do e removido.\n3. **Segunda Divis√£o (Remover P5-P8)**:\n    - Clusters: \\(\\{P1, P2, P3, P4, P5, P6, P7\\}\\) e \\(\\{P8\\}\\)\n    - Como \\(\\{P8\\}\\) tem menos de 5 pontos, √© considerado ru√≠do e removido.\n4. **Terceira Divis√£o (Remover P4-P7)**:\n    - Clusters: \\(\\{P1, P2, P3, P4, P5, P6\\}\\) e \\(\\{P7\\}\\)\n    - Como \\(\\{P7\\}\\) tem menos de 5 pontos, √© considerado ru√≠do e removido.\n5. **Quarta Divis√£o (Remover P3-P6)**:\n    - Clusters: \\(\\{P1, P2, P3, P4, P5\\}\\) e \\(\\{P6\\}\\)\n    - Como \\(\\{P6\\}\\) tem menos de 5 pontos, √© considerado ru√≠do e removido.\n6. **Quinta Divis√£o (Remover P2-P5)**:\n    - Clusters: \\(\\{P1, P2, P3, P4\\}\\) e \\(\\{P5\\}\\)\n    - Como \\(\\{P5\\}\\) tem menos de 5 pontos, √© considerado ru√≠do e removido.\n7. **Sexta Divis√£o (Remover P1-P4)**:\n    - Clusters: \\(\\{P1, P2, P3\\}\\) e \\(\\{P4\\}\\)\n    - Como \\(\\{P4\\}\\) tem menos de 5 pontos, √© considerado ru√≠do e removido.\n\n**Finalizando a Condensa√ß√£o**\n\nAp√≥s remover iterativamente as arestas mais longas e aplicar o crit√©rio de tamanho m√≠nimo do cluster, terminamos com a seguinte estrutura de cluster:\n\n- **Clusters Finalizados**: \\(\\{P1, P2, P3\\}\\)\n\n**Visualiza√ß√£o da √Årvore Condensada**\n\n```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.cluster.hierarchy import dendrogram\n\n# Exemplo de dados para visualiza√ß√£o\nlinkage_matrix = [\n    [0, 1, 1.0, 2],\n    [0, 2, 2.0, 3],\n    [1, 3, 3.0, 4],\n    [2, 4, 4.0, 5],\n]\n\n# Plot do dendrograma\nplt.figure(figsize=(10, 7))\ndendrogram(linkage_matrix, labels=['P1', 'P2', 'P3', 'P4', 'P5'])\nplt.title(\"Dendrograma da Hierarquia de Clusters\")\nplt.xlabel(\"Pontos\")\nplt.ylabel(\"Dist√¢ncia de Alcance M√∫tuo\")\nplt.show()\n\n```\n\n## [6] Extrair os clusters\n\nIntuitivamente, queremos escolher clusters que persistam e tenham uma vida √∫til mais longa; clusters de curta dura√ß√£o s√£o provavelmente apenas artefatos da abordagem de liga√ß√£o √∫nica. Observando o gr√°fico anterior, podemos dizer que queremos escolher aqueles clusters que t√™m a maior √°rea de tinta no gr√°fico. Para fazer uma clusteriza√ß√£o plana, precisaremos adicionar um requisito adicional de que, se voc√™ selecionar um cluster, n√£o poder√° selecionar nenhum cluster que seja descendente dele. E, de fato, essa no√ß√£o intuitiva do que deve ser feito √© exatamente o que o HDBSCAN faz. Claro, precisamos formalizar as coisas para torn√°-las um algoritmo concreto.\n\nPrimeiro, precisamos de uma medida diferente da dist√¢ncia para considerar a persist√™ncia dos clusters; em vez disso, usaremos $\\lambda = \\frac{1}{distance}$ . Para um determinado cluster, podemos definir os valores Œª_birth e Œª_death como o valor de lambda quando o cluster se dividiu e se tornou seu pr√≥prio cluster e o valor de lambda (se houver) quando o cluster se dividiu em clusters menores, respectivamente. Por sua vez, para um dado cluster, para cada ponto *p* nesse cluster, podemos definir o valor Œª_persistence como o valor de lambda no qual esse ponto 'saiu do cluster', que √© um valor entre Œª_birth e Œª_death, pois o ponto sai do cluster em algum momento da vida √∫til do cluster ou deixa o cluster quando o cluster se divide em dois clusters menores. Agora, para cada cluster, compute a **estabilidade** como:\n\n$\\sum_{p\\epsilon cluster}(\\lambda_p-\\lambda_{birth})$\n\nDeclare todos os n√≥s folha como clusters selecionados. Agora, suba pela √°rvore (na ordem inversa da ordena√ß√£o topol√≥gica). Se a soma das estabilidades dos clusters filhos for maior do que a estabilidade do cluster, ent√£o definimos a estabilidade do cluster como a soma das estabilidades dos filhos. Se, por outro lado, a estabilidade do cluster for maior do que a soma de seus filhos, ent√£o declaramos o cluster como um cluster selecionado e desselecionamos todos os seus descendentes. Uma vez que chegamos ao n√≥ raiz, chamamos o conjunto atual de clusters selecionados de nossa clusteriza√ß√£o plana e retornamos isso.\n\n![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/b7f16c67-bf03-415d-918a-e4bf5c83e015/af86156b-ee38-46fe-89b3-4870b165fe6c/Untitled.png)\n\n## GLOSH\n\nOs objetos do clusterizador HDBSCAN tamb√©m suportam o algoritmo de detec√ß√£o de outliers GLOSH. Ap√≥s ajustar o clusterizador aos dados, os escores de outliers podem ser acessados atrav√©s do atributo `outlier_scores_`. O resultado √© um vetor de valores de escores, um para cada ponto de dados ajustado. Escores mais altos representam objetos mais semelhantes a outliers. Selecionar outliers atrav√©s dos quantis superiores √© frequentemente uma boa abordagem.\n\n<aside>\nüìå GLOSH √© um algoritmo de detec√ß√£o de outliers que:\n\n1. Compara a densidade de um ponto com as densidades de outros pontos associados dentro do cluster atual e dos clusters filhos.\n2. Considera como outliers os pontos que t√™m uma densidade substancialmente menor do que o modo de densidade (cluster) com o qual eles mais se associam.\n3. √â calculado a partir de uma hierarquia de clusters, utilizando informa√ß√µes sobre como os clusters \"sobrevivem\" a mudan√ßas nos limiares de densidade.\n</aside>\n\n---\n\n## Em Python\n\n### Par√¢metros do HDBSCAN\n\nAo configurar o HDBSCAN, v√°rios par√¢metros podem ser ajustados para influenciar o comportamento do algoritmo:\n\n- **min_cluster_size**: Define o tamanho m√≠nimo de um cluster. Este par√¢metro determina o n√∫mero m√≠nimo de pontos necess√°rios para formar um cluster.\n- **min_samples**: Define o n√∫mero m√≠nimo de pontos em um vizinho para ser considerado um ponto central. Este par√¢metro afeta a robustez do agrupamento.\n- **metric**: Especifica a m√©trica de dist√¢ncia a ser usada. As op√ß√µes incluem 'euclidean', 'manhattan', 'chebyshev', 'minkowski', entre outras.\n- **cluster_selection_method**: Determina o m√©todo de sele√ß√£o de clusters finais. As op√ß√µes s√£o 'eom' (excess of mass) e 'leaf'. O m√©todo 'eom' √© geralmente recomendado.\n- **alpha**: Um par√¢metro para ajustar a suavidade da hierarquia de densidade. Valores maiores resultam em hierarquias mais suaves.\n- **p**: Par√¢metro de pot√™ncia para a m√©trica de Minkowski. Utilizado apenas se a m√©trica de Minkowski for escolhida.\n\nEsses par√¢metros permitem ajustar o HDBSCAN para diferentes tipos de dados e objetivos de agrupamento.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\na = 1\na\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n1\n```\n:::\n:::\n\n\n",
    "supporting": [
      "hdbscan_files"
    ],
    "filters": [],
    "includes": {}
  }
}